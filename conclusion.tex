\refsec{Conclusion}{conclusion}
\PARstart{I}{t} is believed that the proposed substrate provides a solid base upon which inference of rich \acp{NN} can be run. The architecture has been shown to be of manageable complexity considering other current state of the art \ac{CMOS} circuits and the substrate can be configured for inference on a certain \ac{NN} in a reasonable amount of time.

Since the transistor counts are currently in the same range as a complete \ac{SoC}, it is not deemed probable that the suggested substrate is employed in smartphones with the current state of \ac{CMOS} technology unless only a very small implementation is necessary. The suggested substrate is deemed effective however for efficient inference in larger scale deployments such as in datacenters.

The substrate has been designed to support bi-directionality, mutual lateral inhibition and arbitrary activation functions. Furthermore bias neurons are present and multiple complete chips can be combined to arrive at a larger total network. These functionalities combined provide for a very versatile substrate. Finally weights can be imported to and exported from the substrate to ease deployment and sharing of \ac{NN} implementations.

\refsec{Future work}{future}
While the suggested substrate has been shown to be very versatile, the field of \ac{NN} research is vast and numerous techniques can not yet be executed on the current architecture. These possible enhancements are discussed below and are considered future work. First of all the substrate in the current state is unable to run inference tasks on \acp{RNN}. It is believed that this functionality should be achievable with relatively little effort.

Secondly no learning is possible in the current state of the architecture. It should be possible to implement a separate cycle after the inhibition cycle to allow each neuron to perform a learning strategy. Hebbian learning is suggested initially, since when combined with decay it proves to be a quite efficient approach with the main advantage being that it is local to each neuron and/or synapse. However other training strategies can be considered as well. One major use case for the substrate when training is implemented is that the substrate can be trained without external computations and the learnt \ac{NN} can then be extracted using the already present host processor or USB dongle and distributed. Implementing training inside the substrate will also allow for evolving \acp{NN} with the substrate already in the hands of a consumer.

Further optimizations could be considered, the current substrate is built using high level blocks with little regard for possible micro-optimizations such as eliminating double inverters and omitting redundant hardware, for example when functions are self-dual and inverters are used on both sides\cite[Section 11.2.2.1]{Weste:2010:CVD:1841628}. Further optimizations might include investigating possibilities for transmission gate logic instead of only traditional \ac{CMOS} technology. It must be noted however that most micro-optimizations are probably best handled by automated tools instead of by manual labor.

A final piece of possible future work would be to consider translating the proposed substrate to an analog implementation. While memory and inter-operability are expected to be a large challenge, much is expected to be gained by allowing a smaller implementation and enabling a non-clocked design.
