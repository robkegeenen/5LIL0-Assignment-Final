\refsec{Proposed design overview}{design}
\PARstart{A}{s} detailed in \Cref{sec:related}, most improvement in terms of performance and efficiency is expected to be achieved by highly distributing both the compute power and the memory. In a biological \ac{NN} as found in the (human) brain, the neurons themselves are the computing element and the memory is realized by the synaptic weights between the neurons. \todo[inline]{reference human brain} No central computing element or central memory system is present, the whole brain performs both functions in a highly distributed way. Why would a man-made substrate designed to emulate biological functions behave in a different way? However next to the global requirement that the substrate must be highly distributed, some additional design choices have been made before arriving at the presented architecture.

\refsubsec{Chosen technology}{technology}
\PARstart{R}{egarding} the implementation of any substrate for \acp{NN}, three main strategies exist relating to level of abstraction from biological \acp{NN}. The strategy most closely matching a biological \ac{NN} is the pulse based approach. This approach has every neuron send a pulse train, where the frequency of the pulses roughly relates to the strength of the neuron output. The first abstraction that can be made is to interpret the neuron output strength as an analog value, such as a voltage. This rate code approximation has as main advantage that the next neuron does not have to translate the frequency of pulses into an analog value. While a biological \ac{NN} does not operate this way, little detail is lost by this abstraction.

The second abstraction often made does lose some detail with regards to actual biological \acp{NN}. That approximation is to interpret the neuron outputs as digital signals. The advantage of this abstraction is that much more industry experience exists in large scale, high density digital designs as opposed to analog ones. Another advantage is that memory, for example for synapse weights, is easier to realize for digital signals than it is for analog ones, as are iterative processes leading to equilibria. However the step to the digital domain does have one major drawback, being that all signals become discrete and therefore a limit exists on the amount of detail that can be captured by each signal. Surprisingly often very narrow signals, only 8 bits even, are usually good enough for inference\cite{8192463}. Together with the abstraction to digital signals often also comes the transition to a fully clocked design. Since analog signals can quite easily be left to operate in an unclocked asynchronous way, while this is much harder for digital designs.

Taking these factors into consideration, it has been decided to base the proposed design on a clocked digital strategy, because of the ease of integration with existing technology and because of the ease of implementation of memory. However multiple forays into effective analog substrate implementations are being performed, some of which also produce results usable in the widespread \ac{CMOS} technology, which also eases integration\cite{239745,81862,machofnewsoul}. The author acknowledges that analog approaches could be way more efficient, both in terms of size and in terms of energy. The author therefore endorses further research in this area.

\refsubsec{Layer overview}{layer}
\PARstart{I}{t} is desired that the substrate is able to emulate inference in both \acp{MLP} and \acp{CNN}, since these are two of the most often used \ac{NN} types\cite{8192463}. The ability to also emulate \acp{RNN} would be a nice additional bonus. Supporting \acp{RNN} should be possible in the suggested design with limited modifications. \todo[inline]{make this happen} The suggested implementation consists of a fully connected network in its base. This ensures it is able to emulate both \acp{MLP} and \acp{CNN}.

The global overview of a single layer is presented in \todo[inline]{insert \textbackslash Cref\{\}}. The \texttt{N\_l\_*} blocks are the neurons, the implementation of which will be detailed in \Cref{sec:neuron}. The \texttt{A\_l} block performs the activation function for all neurons in this layer. Its implementation will be explained in \Cref{sec:activation}. Finally the \texttt{B\_l} block is the bias neuron and it will be described in more detail in \Cref{sec:bias}. Multiple of these layers can be connected together via the \signal{layer\_in} and \signal{layer\_out} ports and each neuron in this layer will be able to communicate to all neurons in the next layer and to all neurons in the previous layer, achieving a fully connected network.

In a biological \ac{NN} each neuron is connected to a number of other neurons and each connection happens over an individual synapse. The strengths of these synapse connections are the cause of the weights between the neurons. In the digital implementation an analogy can be made between the synapses being represented by a weight and a multiplier, while the cell bodies of the neurons can be represented by accumulators. Implementing a single multiplier for each set of artificial neurons to be connected would use up a lot of space. Statically connecting all combinations of neurons would also be an architectural nightmare. Therefore it has been decided that the weights and multipliers are merged with the accumulator into the receiving neurons. The neurons will then be connected over a shared bus between layers the neurons in a layer are then iterated over to still achieve full connectivity. This reduces the number of multipliers needed and the routing troubles tremendously, obviously at the cost of speed.

The \signal{external} port is connected in parallel to all layers, except for the token connections \signal{Toki} and \signal{Toko}, and will be brought to the outside of the chip. This port contains signals that are important to all blocks, such as the chip identification (\signal{CID}), the clock and reset lines. The \signal{Toki} and \signal{Toko} signals can be used to combine multiple chips together to achieve a network with more neurons per layer, the \signal{Enable} signal can be used to enable only one bias neuron in the combined layer in this case. If this feature is not desired, the token signals can be connected together. Furthermore the \signal{read\_enable}, \signal{write\_enable}, \signal{read\_addr}, \signal{write\_addr}, \signal{read\_data} and \signal{write\_data} signals allow the first and the last layers to communicate, either to a host processor or a memory located next to the substrate. This communication is required for feeding data into and retrieving data out of the \ac{NN}. These signals are left unconnected on other layers inside a single chip.

The \signal{config\_*} signals are important in configuring the parameters for the \ac{NN} and they function as a large shift register, shifting data in on the \signal{config\_in} line at each clock tick on the \signal{config\_clk} line. The \signal{config\_out} line can be daisy-chained to other layers, causing the complete chip to become one large shift register and only needing three signals to the outside to configure all parameters of the \ac{NN}. This approach might sound reminiscent of certain boundary scan techniques such as the one employed in daisy-chained JTAG (IEEE 1149.1)\todo[inline]{insert \textbackslash cite\{\}}, which the configuration system has taken inspiration of. The configuration system in this form allows for either a host processor located next to the substrate, or a light-weight dongle like a USB adapter to be used for configuration. This system also allows more than one substrate chip to be daisy-chained on a single \ac{PCBA}.

\refsubsec{Cycle overview}{cycles}
\PARstart{D}{uring} the operation of the substrate, several cycles are present that each component goes through. These present cycle is indicated by one of the \signal{Fwd}, \signal{Bwd} and \signal{Ihib} signals. Where the \signal{Fwd} signal indicates that the substrate is in the forward propagating cycle, where the outputs of neurons in one layer are being fed to the neurons in the next layer. The \signal{Bwd} signal indicates that the substrate is in the backward propagating cycle, where the outputs of neurons in one layer are being fed back to the neurons in the previous layer. Finally the \signal{Ihib} signal indicates that the substrate is in the inhibition cycle, where the neurons of each layer are being inhibited by a \ac{kWTA} algorithm. This algorithm has been chosen since it is easy to implement among all neurons of a single layer and it provides a good approximation for biological \acp{NN}, even though the computational mechanism is biologically implausible\cite{emergentkwta}.

The forward and backward cycles enable the substrate to be used for inference on bi-directional \acp{NN}, while the inhibition cycle allows for lateral inhibition in \acp{NN} that require it. While forward inhibition is sometimes used in artificial \acp{NN}, the substrate will only allow for lateral inhibition since it represents biological \acp{NN} more closely and is easier to implement.

During each cycle, all of the neurons in each layer are iterated over by means of a token that passes through each neuron via the \signal{Toki} and \signal{Toko} signals. In this way each neuron is active for one clock tick during each of the three cycles. The generation of the cycles is handled by the bias neuron as described in \Cref{sec:bias}, between each cycle the token is held by the bias neuron for one clock tick, which gives the neurons the chance to propagate their accumulated value for that cycle to the output as is described in \Cref{sec:neuron}.

\refsubsec{Neuron implementation}{neuron}
\PARstart{T}{he} implementation of a single neuron is shown in \todo[inline]{insert \textbackslash Cref\{\}}. Each neuron has an address, which is based on both the \signal{CID} signal coming from outside the chip and an internal hardwired address part. This ensures that each neuron in a layer has a unique address, even if multiple chips are combined to create layers with more neurons.

Once a neuron is active in the forward cycle this address is put on the output bus, just as the activation strength gathered in the previous cycle. Meanwhile during all clock ticks in the forward cycle the address of the selected neuron in the previous layer is used to select the correct synapse weight from \texttt{regcf\_fwd}, which is then multiplied with the activation strength of that neuron after it has passed through the activation function as described in \Cref{sec:activation}. This value is then accumulated in \texttt{reg\_val}. During the extra clock tick after each cycle this value is propagated from \texttt{reg\_val} to \texttt{reg\_prev}, readying \texttt{reg\_val} for the backward cycle.

During the backward cycle the process is quite similar to the forward cycle except the activation strength of neurons in the next layer is used and the synapse weights are fetched from \texttt{regcf\_bwd} instead. Again during the extra clock tick after each cycle the accumulated value is propagated to \texttt{reg\_prev}, readying \texttt{reg\_val} for the inhibition cycle.

During the inhibition cycle each neuron in a single layer outputs its activation strength, all other neurons then determine if they are activated more strongly or not, by means of \texttt{alu\_gt}. If a neuron is more strongly activated, \texttt{cnt\_ihib} is incremented. As soon as \texttt{cnt\_ihib} contains a value greater than \texttt{regc\_ihib\_k}, \texttt{reg\_val} is reset to a configurable value from \texttt{regc\_ihib\_rst}. This mechanism achieves a \ac{kWTA} type lateral inhibition in which the value of $k$, which is the number of neurons maximally active at the same time, can be configured via \texttt{regc\_ihib\_k}. During the inhibition cycle the activation strength of the neurons does not pass through the activation layer, since that would mean the neurons also have to calculate the inverse of this function in order to compare strengths, which introduces a lot of complications. The chosen solution is valid since the activation function is shared among all neurons in a layer, however the result of the \ac{kWTA} mechanism is only correct for monotonically increasing activation functions. During the extra clock tick after each cycle the value from \texttt{reg\_val}, either reset or not based on the \ac{kWTA} mechanism, is transferred to \texttt{reg\_prev}, readying \texttt{reg\_val} for the forward cycle again.

Regarding configuration, all configurable elements \texttt{regcf\_fwd}, \texttt{regcf\_bwd}, \texttt{regc\_ihib\_rst} and \texttt{regc\_ihib\_k} are wired in series to keep up the massive shift register paradigm.

It can be noted that neurons could be multiplexed, in the sense that instead of one value register and a previous value register, two register files could be employed. The advantage of this scheme would be that one neuron could behave as if it were multiple neurons, this would lead to the possibility of running inference on larger \acp{NN} with less extra hardware. However a $8$-to-$1$ demultiplexer needed for implementing the register files already costs around four thousand transistors, while an extra $16$x$16$-bits multiplier only costs around six thousand transistors. This means that the overhead of implementing this scheme would become inefficient quite quickly.

\refsubsec{Activation function implementation}{activation}
\PARstart{T}{he} implementation of the activation function is quite simply a relatively large amount of configurable memory acting as a \ac{LUT}, for which the address is taken as the raw activation strength of the currently selected neuron, see \todo[inline]{insert \textbackslash Cref\{\}}. The data output then represents the output of the activation function with the raw activation strength as input. Since a large amount of memory takes up valuable space and the need to iterate over the neurons in a layer already exists due to the complexity involved in connecting the neurons in a $n$-by-$n$ fashion, it has been decided to employ a single activation function per layer. The activation function is shared among all neurons in a layer by means of a shared bus.

During the forward and backward cycles the activation function operated by taking the raw activation value in and outputting the value modified by the function programmed in the memory. The only difference is that it outputs towards the next layer during the forward cycle and it outputs towards the previous layer during the backward cycle. During the inhibition cycle the activation function is idle, since inhibition is implemented without taking the activation function into account as discussed in \Cref{sec:neuron}.

Since the activation function is implemented as a large \ac{LUT}, it is not only possible to use the two most often used activation functions in artificial \acp{NN} \ac{ReLU} and Sigmoid\cite{8192463}, but also any other arbitrary function can be used as activation function.

\refsubsec{Bias implementation}{bias}
\PARstart{I}{n} numerous artificial \acp{NN} a bias neuron is present to provide a bias value to the next layer, which allows for shifting the activation function over the input domain\todo[inline]{insert \textbackslash cite\{\}}. While the implementation of the activation function allows any arbitrary function to be used, that function is still shared among all neurons in a layer. Introducing a bias neuron which has individual weights in each receiving neuron, just like a normal sending neuron, allows the activation function to be tweaked per neuron. The bias neuron is special however in that it is implemented in the same layer it is giving bias to, instead of in the previous layer as is common in other artificial \acp{NN}. This is done to ensure that each layer has a bias neuron. The bias is simply implemented by a configurable value that is outputted during the forward and backward cycles if the bias neuron has the token.

Another duty that has been entrusted to the bias neuron is the generation of the cycle signals and the generation of the initial token. These tasks are implemented in the bias neuron, since they have to be global for the whole layer. The generation of the cycle signals is achieved by a three step shift register that is advanced each time the bias neuron gets the token. The initial token is implemented by presetting the first token flip-flip in the bias neuron instead of resetting it on reset. Some additional buffers and a multiplexer are present to enable the bias neuron to be disabled in case multiple chips are connected together, since only one bias neuron should be active in a given layer.
