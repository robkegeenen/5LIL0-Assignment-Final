%\begin{abstract}
%    This is the abstract
%\end{abstract}
%\acresetall

\refsec{Introduction}{intro}
\PARstart{I}{nterest} in artificial \acp{NN} is rising rapidly in the current day and age. A lot of research is being performed on \acp{NN} and they have even propagated trough to the consumer market. Nowadays \ac{NN} processing units can be found in a number of flagship smartphones. Almost all, if not all, of the present solutions still rely on the old paradigm of one or more relatively general purpose processors, often \acp{CPU} or \acp{GPU}, running firmware or software geared towards running \ac{NN} simulations, as that is what they can be regarded as, as long as the hardware does not reflect the structure of the \ac{NN} on a very low level. Major performance and efficiency benefits are expected to be achievable by tuning the underlying hardware so that it will reflect the structure of an artificial \ac{NN} more closely. This paper will explore one possible implementation of a \ac{NN} substrate developed with implementation in industry standard CMOS technology in mind. First the intended design setup will be detailed, after which the achievability will be described along with the expectations on performance and efficiency characteristics.

Two phases can be distinguished during the deployment of \acp{NN}, namely the \textit{training} (or learning) phase and the \textit{interference} (or prediction) phase\cite{8192463}. The training phase is where the \ac{NN} is trained to perform a certain task. In a sense it is educated, or raised. The interference phase on the other hand, is the phase where the \ac{NN} actually performs its task, such as recognizing objects in images or translating speech to text. While the training phase is evidently very important, since without it \acp{NN} would never perform any useful function, the implementation in this paper focuses on enhancing the performance of the inference phase, since that is the phase the customer will have to deal with when a \ac{NN} is deployed in a consumer electronics device. It is not unthinkable that in the future the learning phase may also be (partially) supported by the suggested design, and some thoughts are voiced regarding this in \Cref{sec:future}.

\refsec{Related works}{related}
% (and why they suck balls)
\PARstart{N}{eural} Processing Engines intended for running \acp{NN} do exist, and are also becoming more common in flagship smartphones of certain manufacturers. It is expected that it will not take too long before these processing units will be commonplace even in budget smartphones and other low-cost consumer electronic devices. Huawei released their Kirin 970 \ac{SoC} on the 2\textsuperscript{nd} of September 2017\cite{huawei}, including a ``Neural Processing Unit''. On the 9\textsuperscript{th} of August 2018 Qualcomm announced their Snapdragon 845 \ac{SoC} which will be used in the Samsung Galaxy Note9 in selected regions\cite{qualcomm}, also this \ac{SoC} includes an ``AI Engine''. When Apple announced their new flagship phone, the iPhone X, on the 12\textsuperscript{th} of September 2017, they also revealed that it would be powered by the A11 Bionic \ac{SoC} developed by themselves\cite{apple}. The A11 Bionic also includes a ``Neural Engine''. While all these accelerators claim to improve the performance of \ac{AI} related tasks, they are in fact still relatively general massively parallel processing units\cite{techtarget}.

One implementation of a \ac{NN} accelerator chip which stretches these bounds quite a bit is the Google developed Tensor Processing Unit\cite{8192463,techtarget}. While this technology is clearly geared towards matrix multiplication and accumulation, which are the core functions underlying all \acp{NN}, it still relies heavily on centralized control and memory. The technology has limited memory inside the chip and relies on external memory chips to provide the storage for the neural weights.

Also Intel is stepping into the game of Neural Processing Engines by their acquirement of Movidius Inc. with their Myriad X device\cite{movidius2}. While few details of the internal architecture of the Neural Compute Engine inside this device is given, it is expected to also build solely upon massively parallel processing, judging by the architecture of the previous generation Myriad devices\cite{movidius1}. Another drawback that is certainly still present in the new generation of devices is that they still rely on large amounts of centralized memory.

Efforts have been made in pursuit of a substrate specifically designed for running \acp{NN}\cite{1693534} and also designed for more general massively parallel computing\cite{7818353}. These efforts are however usually still based on a re-configurable array of certain functional elements, this means that configuration overhead will be non-negligible and it means that the architecture is not able to be optimized solely for running \acp{NN}.

The implementation suggested in this paper will try to minimize these drawbacks by
\begin{itemize}
  \item introducing neuron entities that will mimic the functionality of a biological neurons,
  \item distributing the compute power over the neuron entities, instead of applying a centralized compute paradigm,
  \item distributing the memory over the neuron entities, instead of applying a centralized memory paradigm,
  \item minimizing the centralized control necessary to operate the network and
  \item dis-allowing reconfiguration of the architecture, only parameters will be configurable.
\end{itemize}
These differences will produce an architecture that more closely models the way biological \acfp{NN} function, therby hopefully achieving better performance and efficiency.
