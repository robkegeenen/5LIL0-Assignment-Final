Conclusion:
 - MLP (not only perceptrons) and CNN highest prio, RNN lower prio \cite{8192463}
   - Fully connected for highest adaptability
 - Forward propagation (inference)
 - Backward propagation (inference)
 - Mutual inhibition (inference) (causes cycles?) (https://en.wikipedia.org/wiki/Lateral_inhibition)
 - Activation function (inference) (Sigmoid + Rectifier?)
 - Hebbian learning (+decay?, x per cycle lost) (Training) (Considered future work)
 - Bias neurons
 - Other learning (Training) (no, is too complex)
 - Weight import/export
 - Daisy chained chips
 


Optimizations:
  11.2.2.1, Weste:2010:CVD:1841628, This delay can be reduced by omitting the inverters on the outputs, as was done in Figure 11.4(c). Because addition is a self-dual function (i.e., the function of complementary inputs is the complement of the function)
  Pass-function, pass-gates, transmission gate
  Merging double inversions, except where used as buffer.
  
  Best done at least partially by automated tools.


Future work:
  - translate design to analog
  - implement learning => learnt model can be exported using same USB dongle/host proc.

